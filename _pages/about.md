---
layout: about
title: Home
permalink: /
subtitle: <strong><font size="4">AI/ML Researcher</font></strong>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <a href="https://scholar.google.com/citations?user=NjhpUykAAAAJ">Google Scholar</a>, <a href="https://github.com/aounon">GitHub</a>, <a href="https://www.linkedin.com/in/aounon-kumar/">LinkedIn</a>

news: false # includes a list of news items
latest_posts: false # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

I am a Postdoctoral Research Associate at Harvard University working in `Trustworthy AI` with Professor [Himabindu Lakkaraju](https://himalakkaraju.github.io/). My research focuses on the `robustness`, `security`, and `safety` of machine learning (ML) models. It involves designing algorithms to defend models against adversarial inputs, for example, safeguarding large language models (LLMs) from prompts that circumvent safety guardrails. I have studied and contributed to model robustness in several machine learning domains including `computer vision`, `reinforcement learning`, and `language modeling`. My work has been accepted in prominent ML conferences such as `ICML`, `ICLR` and `NeurIPS`, and I am actively involved in collaborative projects within the academic community.

**Media Coverage:** My recent works on `LLM robustness` and `reliability` have been featured in major media and academic news outlets:
  1. [Science News Magazine](https://www.sciencenews.org/article/generative-ai-chatbots-chatgpt-safety-concerns?trk=feed_main-feed-card_feed-article-content), [D^3 Institute at Harvard](https://d3.harvard.edu/certifying-llm-safety-against-adversarial-prompting/). Work featured: [Certifying LLM Safety against Adversarial Prompting](https://openreview.net/forum?id=9Ik05cycLq).
  2. [The New York Times](https://www.nytimes.com/2024/08/30/technology/ai-chatbot-chatgpt-manipulation.html). Work featured: [Manipulating Large Language Models to Increase Product Visibility](https://arxiv.org/abs/2404.07981).
  3. [The Washington Post](https://www.washingtonpost.com/technology/2023/06/02/turnitin-ai-cheating-detector-accuracy/), [Bloomberg](https://www.bloomberg.com/news/newsletters/2023-11-06/biden-ai-executive-order-shows-urgency-of-deepfakes), [Wired](https://www.wired.com/story/ai-detection-chat-gpt-college-students/), [New Scientist](https://www.newscientist.com/article/2366824-reliably-detecting-ai-generated-text-is-mathematically-impossible/), [The Register](https://www.theregister.com/2023/03/21/detecting_ai_generated_text/), [TechSpot](https://www.techspot.com/news/98031-reliable-detection-ai-generated-text-impossible-new-study.html). Work featured: [Can AI-Generated Text be Reliably Detected?](https://arxiv.org/abs/2303.11156).

Before joining Harvard, I completed my PhD at the [University of Maryland](https://www.cs.umd.edu) in `certified robustness` in machine learning (see my dissertation [here](https://drum.lib.umd.edu/items/f4ad78d5-f6a8-47cf-bdca-358410186a96)). I was fortunate to be advised by Professors [Soheil Feizi](https://www.cs.umd.edu/~sfeizi/) and [Tom Goldstein](https://www.cs.umd.edu/~tomg/). During my PhD, I have spent time as a research intern at [Nokia Bell Labs](https://www.bell-labs.com/) and an applied scientist intern at [Amazon](https://www.amazon.science/), where I worked on network security-related machine learning applications and uncertainty estimation for human action recognition models. I have also served as a reviewer for machine learning conferences such as ICLR and NeurIPS.

I did my undergraduate studies at [IIT Mandi](https://iitmandi.ac.in/) and my master's at [IIT Delhi](https://home.iitd.ac.in/), where I studied a wide range of topics in computer science such as machine learning, advanced algorithms, combinatorial optimization, complexity theory and cryptography. My master's thesis was on the computational hardness of approximating the optimal solution of a variant of the k-center clustering problem.

<br>

## News

| | |
| **Aug 30, 2024** &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | :page_facing_up: New pre-print on [Manipulating Large Language Models to Increase Product Visibility](https://arxiv.org/abs/2404.07981)! Covered by [The New York Times](https://www.nytimes.com/2024/08/30/technology/ai-chatbot-chatgpt-manipulation.html). |
|  |  |
| **Jul 10, 2024** &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Accepted at COLM 2024 :llama:: [Certifying LLM Safety against Adversarial Prompting](https://openreview.net/forum?id=9Ik05cycLq). Covered by [Science News Magazine](https://www.sciencenews.org/article/generative-ai-chatbots-chatgpt-safety-concerns?trk=feed_main-feed-card_feed-article-content). |
|  |  |
| **Dec 19, 2023** | Graduated from `UMD`! :man_student: |

<!-- table>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Feb 12, 2024</strong></td>
    <td> New pre-print on <a href="https://arxiv.org/abs/2309.02705">Certifying LLM Safety against Adversarial Prompting</a>. Covered by <a href="https://www.sciencenews.org/article/generative-ai-chatbots-chatgpt-safety-concerns?trk=feed_main-feed-card_feed-article-content">Science News Magazine</a>.
    </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Dec 19, 2023</strong></td>
    <td>Graduated from `UMD`! :man_student:</td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Oct 05, 2023</strong></td>
    <td>Started as a Research Associate at `Harvard University`.</td>
  </tr>
</table -->

<br>

## Selected Publications
See full list at [Google Scholar](https://scholar.google.com/citations?user=NjhpUykAAAAJ).

<table>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>COLM 2024</strong></td>
    <td><strong>Certifying LLM Safety against Adversarial Prompting</strong><br>
        <b>Aounon Kumar</b>, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, Himabindu Lakkaraju<br>
        <a href="https://openreview.net/forum?id=9Ik05cycLq">OpenReview</a>, <a href="https://arxiv.org/abs/2309.02705">ArXiv</a>, <a href="https://github.com/aounon/certified-llm-safety">Code</a>, <a href="https://arxiv.org/pdf/2309.02705.pdf">PDF</a><br>
        Media Coverage: <a href="https://www.sciencenews.org/article/generative-ai-chatbots-chatgpt-safety-concerns?trk=feed_main-feed-card_feed-article-content">Science News Magazine</a>, <a href="https://d3.harvard.edu/certifying-llm-safety-against-adversarial-prompting/">D^3 Institute at Harvard</a>.
    </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Preprint</strong></td>
    <td><strong>Manipulating Large Language Models to Increase Product Visibility</strong><br>
        <b>Aounon Kumar</b>, Himabindu Lakkaraju<br>
        <a href="https://arxiv.org/abs/2404.07981">ArXiv</a>, <a href="https://github.com/aounon/llm-rank-optimizer">Code</a>, <a href="https://arxiv.org/pdf/2404.07981">PDF</a><br>
        Media Coverage: <a href="https://www.nytimes.com/2024/08/30/technology/ai-chatbot-chatgpt-manipulation.html">The New York Times</a>.
    </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>Preprint</strong></td>
    <td><strong>Can AI-Generated Text be Reliably Detected?</strong><br>
        Vinu Sankar Sadasivan, <b>Aounon Kumar</b>, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi<br>
        <a href="https://arxiv.org/abs/2303.11156">ArXiv</a>, <a href="https://github.com/vinusankars/Reliability-of-AI-text-detectors">Code</a>, <a href="https://arxiv.org/pdf/2303.11156.pdf">PDF</a><br>
        Media Coverage: <a href="https://www.washingtonpost.com/technology/2023/06/02/turnitin-ai-cheating-detector-accuracy/">The Washington Post</a>, <a href="https://www.bloomberg.com/news/newsletters/2023-11-06/biden-ai-executive-order-shows-urgency-of-deepfakes">Bloomberg</a>, <a href="https://www.wired.com/story/ai-detection-chat-gpt-college-students/">Wired</a>, <a href="https://www.newscientist.com/article/2366824-reliably-detecting-ai-generated-text-is-mathematically-impossible/">New Scientist</a>, <a href="https://www.theregister.com/2023/03/21/detecting_ai_generated_text/">The Register</a>, <a href="https://www.techspot.com/news/98031-reliable-detection-ai-generated-text-impossible-new-study.html">TechSpot</a>.
    </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>ICLR 2023</strong></td>
    <td><strong>Provable Robustness against Wasserstein Distribution Shifts via Input Randomization</strong><br>
        <b>Aounon Kumar</b>, Alexander Levine, Tom Goldstein, Soheil Feizi<br>
        <a href="https://openreview.net/forum?id=HJFVrpCaGE">OpenReview</a>, <a href="https://arxiv.org/abs/2201.12440">ArXiv</a>, <a href="https://github.com/aounon/distributional-robustness">Code</a>, <a href="https://arxiv.org/pdf/2201.12440.pdf">PDF</a>
    </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>ICLR 2022</strong></td>
    <td><strong>Policy Smoothing for Provably Robust Reinforcement Learning</strong><br>
        <b>Aounon Kumar</b>, Alexander Levine, Soheil Feizi<br>
        <a href="https://openreview.net/forum?id=mwdfai8NBrJ">OpenReview</a>, <a href="https://arxiv.org/abs/2106.11420">ArXiv</a>, <a href="https://openreview.net/attachment?id=mwdfai8NBrJ&name=supplementary_material">Code</a>, <a href="https://arxiv.org/pdf/2106.11420.pdf">PDF</a>
    </td>
  </tr>
  <tr><td><br></td><td><br></td></tr>
  <tr>
    <td valign="top" width="20%"><strong>ICML 2020</strong></td>
    <td><strong>Curse of Dimensionality on Randomized Smoothing for Certifiable Robustness</strong><br>
        <b>Aounon Kumar</b>, Alexander Levine, Tom Goldstein, Soheil Feizi<br>
        <a href="https://proceedings.mlr.press/v119/kumar20b.html">ICML Proceedings</a>, <a href="https://arxiv.org/abs/2002.03239">ArXiv</a>, <a href="https://github.com/alevine0/smoothingGenGaussian">Code</a>, <a href="https://arxiv.org/pdf/2002.03239.pdf">PDF</a>
    </td>
  </tr>
</table>

<br>

## Contact

[Science and Engineering Complex](https://seas.harvard.edu/about-us/visit-us/allston/science-engineering-complex) :office:<br>
150 Western Ave<br>
Office #6220<br>
Allston, MA 02134

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](https://fontawesome.com/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->
